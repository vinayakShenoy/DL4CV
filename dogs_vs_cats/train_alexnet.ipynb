{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import import_ipynb\n",
    "from config import dogs_vs_cats_config as config\n",
    "from pyimage.preprocessing.preprocessors import ImageToArrayPreprocessor, SimplePreprocessor, PatchPreprocessor, MeanPreprocessor\n",
    "from pyimage.callbacks import TrainingMonitor\n",
    "from pyimage.io.hd5py import HDF5DatasetGenerator\n",
    "from pyimage.nn.alexnet import AlexNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizer import Adam\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "                        width_shift_range=0.2, height_shift_range=0.2,\n",
    "                        shear_range=0.15, horizontal_flip=True,\n",
    "                        fill_mode=\"nearest\")\n",
    "\n",
    "means = json.loads(open(config.DATASET_MEAN)).read()\n",
    "\n",
    "sp = SimplePreprocessor(227, 227)\n",
    "pp = PatchPreprocessor(227, 227)\n",
    "mp = MeanPreprocessor(mean[\"R\"], means[\"G\"], means[\"B\"])\n",
    "iap = ImageToArrayPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training dataset generator. Here we supply the path to our training\n",
    "# HDF5 file, indicating that we should use batch sizes of 128 images, data augmentation, and three\n",
    "# pre-processors: patch, mean, and image to array, respectively.\n",
    "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 128, aug=aug,\n",
    "                               preprocessors=[pp, mp, iap], classes=2)\n",
    "\n",
    "# for instantiating the testing generator. This time weâ€™ll supply\n",
    "# the path to the validation HDF5 file, use a batch size of 128, no data augmentation, and a simple\n",
    "# pre-processor rather than a patch pre-processor (since data augmentation is not applied to validation\n",
    "# data). \n",
    "valGen = HDF5DatasetGenerator(config.VAL_HDF5, 128,\n",
    "                             preprocessors=[sp, mp, iap], classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the optimizer\n",
    "print(\"INFO compiling model\")\n",
    "opt = Adam(lr=1e-3)\n",
    "model = AlexNet.build(width=227, height=227, depth=3, classes=2, reg=0.002)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "path = os.path.sep.join([config.OUTPUT_PATH, \"{}.png\".format(os.getpid())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit_generator(\n",
    "    trainGen.generator(),\n",
    "    steps_per_epoch=trainGen.numImages//128,\n",
    "    validation_data=valgen.generator(),\n",
    "    validation_steps=valGen.numImages//128,\n",
    "    epochs=75,\n",
    "    max_queue_size=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INFO serializing model\")\n",
    "model.save(config.MODEL_PATH, overwrite=True)\n",
    "\n",
    "trainGen.close()\n",
    "valGen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 75), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 75), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 75), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 75), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
