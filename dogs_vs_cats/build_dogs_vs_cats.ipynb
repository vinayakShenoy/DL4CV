{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import import_ipynb\n",
    "from config import dogs_vs_cats_config as config\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyimage.preprocessing.preprocessors import AspectAwarePreprocessor\n",
    "from pyimage.io.hdf5py import HDF5DatasetWriter\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import json\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep():\n",
    "\n",
    "  # grab the paths to the images\n",
    "  trainPaths = list(paths.list_images(config.IMAGES_PATH))\n",
    "\n",
    "  # image filename is of the format train/{class}.{#IMAGE}.jpg\n",
    "  trainLabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in trainPaths]\n",
    "  le = LabelEncoder()\n",
    "  trainLabels = le.fit_transform(trainLabels)\n",
    "\n",
    "\n",
    "  # perform stratified sampling from the training set to build the testing\n",
    "  # split from th training data\n",
    "  split = train_test_split(trainPaths, trainLabels, test_size=config.NUM_TEST_IMAGES, \n",
    "                           stratify=trainLabels, random_state=42)\n",
    "  (trainPaths, testPaths, trainLabels, testLabels) = split\n",
    "\n",
    "  # perform another stratified sampling this time to build the validation data\n",
    "  split = train_test_split(trainPaths, trainLabels,\n",
    "                          test_size=config.NUM_VAL_IMAGES, stratify=trainLabels,\n",
    "                          random_state=42)\n",
    "  (trainPaths, valPaths, trainLabels, valLabels) = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    \n",
    "    \n",
    "  # construct a list pairing the training, validation, and testing\n",
    "  # image paths along with their corresponding labels and output HDF5\n",
    "  # files\n",
    "  datasets = [\n",
    "      (\"train\", trainPaths, trainLabels, config.TRAIN_HDF5),\n",
    "      (\"val\", valPaths, valLabels, config.VAL_HDF5),\n",
    "      (\"test\", testPaths, testLabels, config.TEST_HDF5)\n",
    "  ]\n",
    "\n",
    "  # init the image preprocessor and list of RGB channel averages\n",
    "  aap = AspectAwarePreprocessor(256, 256)\n",
    "  (R, G, B) = ([], [], [])\n",
    "\n",
    "  # build the HDF5 dataset\n",
    "  for (dType, paths, labels, outputPath) in datasets:\n",
    "      #create HDF5 writer\n",
    "      print(\"INFO building {}...\".format(outputPath))\n",
    "    writer = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
    "\n",
    "    # init the progressbar\n",
    "    widgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",\n",
    "              progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(paths), widgets=widgets).start()\n",
    "\n",
    "    # loop over image paths\n",
    "    for (i, (path, label)) in enumerate(zip(paths, labels)):\n",
    "      image = cv2.imread(path)\n",
    "      image = aap.preprocess(image)\n",
    "\n",
    "      # if we are building the training dataset, compute mean of\n",
    "      # each channel in image, the update respective lists\n",
    "      if dType==\"train\":\n",
    "        (b,g,r) = cv2.mean(image)[:3]\n",
    "        R.append(r)\n",
    "        G.append(g)\n",
    "        B.append(b)\n",
    "\n",
    "      # add image and label # to HDF5 dataset\n",
    "      writer.add([image], [label])\n",
    "      pbar.update[i]\n",
    "\n",
    "    #close HDF5 writer\n",
    "    pbar.finish()\n",
    "    writer.close()\n",
    "\n",
    "  # The final step is to serialize our RGB averages to disk\n",
    "  print(\"INFO serializing means...\")\n",
    "  D = {\"R\": np.mean(R), \"G\":np.mean(G), \"B\":np.mean(B)}\n",
    "  f = open(config.DATASET_MEAN, \"w\")\n",
    "  f.write(json.dumps(D))\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
