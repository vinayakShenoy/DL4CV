{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hdf5py.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9CywE3xnHcBtn15014mAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayakShenoy/DL4CV/blob/master/Code/pyimage/io/hdf5py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-7OLe_W9C6Q",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction\n",
        "- Next few chapters will be on the concept of transfer learning, the ability to use a pretrained model as a shortcut to learn patterns from data it was not originally trained on.\n",
        "- There are two types of transfer learning when applied to deep learning for computer vision:\n",
        "  - Tresting networks as arbitrary feature extractors.\n",
        "  - Removing the fully-connected layers of an existing network, placing new FC layer set on top of CNN and fine-tuning these weights(and optionally previous layers) to recognize object classes\n",
        "- This chapter, we discuss first method, i.e, treating networks as feature extractors.\n",
        "\n",
        "## Extracting features with a Pre-trained CNN\n",
        "- Usually, we treat CNN as end-to-end image classifiers.\n",
        "  - We input an image to the network\n",
        "  - The image forward propagates through the network.\n",
        "  - We obtain the final classification probabilities from the end of the network.\n",
        "- We can stop the propagation at an arbitrary layer and extract the values at this time and use them as feature vectors, that quantifies the contents of an image.\n",
        "- If this repeated for an entire dataset, we cana train SVM, Logistic regression, or random forest on top of these features to obtain a classifier that recognizer new class of images.\n",
        "-the trick is extracting these features and storing them in an efficient manner. To accomplish this task, we’ll need HDF5.\n",
        "\n",
        "## HDF5\n",
        "- HDF5 is a binary data format used to store gigantic datasets on disk while facilitating easy access and computation on rows of the datasets. \n",
        "- Written in C, but we can gain access to C API using Python library, allowing us store huge amounts of data in our HDF5 dataset and manipulate the data in a numpy-like fashion.\n",
        "- Datasets stored in HDF5 format is portable and can be accessed in C, Matlab and java.\n",
        "- Below we will write a custom python class that allows us to efficiently accept input data and write it to HDF5 dataset. \n",
        "  - Facilitate method to apply transfer learning by taking extracted features from VGG16 and writing them to HDF5.\n",
        "  - Allow us to generate HDF5 datasets from raw images to facilitate faster training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "381-yoHwWhfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a5147fbc-39c3-4e31-af96-0cc314fb21bc"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk7tPKir84a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1X3Fs3gOL-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5DatasetWriter:\n",
        "  # dims parameter control dimensions of data that will be stored in dataste.\n",
        "  #     If we are storing flatten raw pixel intensities of 28x28=784, for 70000 examples then dims=(70000, 784)\n",
        "  #     If we store raw CIFAR10 (unflattened), dims=(60000, 32, 32, 3)\n",
        "  #     In context of feature extraction, if the final POOL layer is 512x7x7 when flatten, it is a feature vector of length 25088. We set dims=(N, 25088) \n",
        "  #     where N is number of images in dataset\n",
        "  # datakey indicates that we are storing extracted features from CNN.\n",
        "  # bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset\n",
        "\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\", bufSize=1000):\n",
        "    #check to see if output path exists\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"Path exists\", outputPath)\n",
        "\n",
        "    #copen hdf5 db for writing and create two datasets\n",
        "    #one to store images and another to store class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    self.data = self.db.create_dataset(dataKey, dims, dtype=\"float\")\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n",
        "\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\":[], \"labels\":[]}\n",
        "    self.idx = 0\n",
        "  \n",
        "  def add(self, rows, labels):\n",
        "    # add rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "  \n",
        "  def flush(self):\n",
        "    # write buffers to disk then reset buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\":[], \"labels\":[]}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label  names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=unicode)\n",
        "    labelSet = self.db.create_dataset(\"label_names\", (len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check if there are any entries in buffer \n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]>0):\n",
        "      self.flush()\n",
        "      self.db.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}