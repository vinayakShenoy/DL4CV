{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GoogleNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkaMAm5G4YCsGkKlANdrV+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QCeVnC4B_KnD","colab_type":"text"},"source":["# GoogleNet\n","- Model is tiny compared to AlexNet and VGGNet, as the authors are able to obtain a dramatic drop in netork architecture size by removed fully-connected layers and instead using global average pooling.\n","- The architecture also makes use of network in network or micro-architecture when constructing the overall macro-architecture.\n","- Contributed the Inception module to the deep learning community, a\n","building block that fits into a Convolutional Neural Network enabling it to learn CONV layers with multiple filter sizes, turning the module into a multi-level feature extractor.\n","\n","## Inception Module and its variants\n","- The general idea behind inception module is two-fold:\n","  - Should the filter size be 5x5, 3x3 or 1x1 (which ca learn local features)? Inception module learns all three filters by computing them in parallel anc concatenating the resulting feature maps along the channel dimension. The next layer in GoogleNet  architecture recieves these concatenated, mixed filters and performs the same process. This process enables GoogleNet to learn both local features via smaller convolution and abstracted features with larger convolution.\n","  - By learning multiple filter sizes, we can turn the module into a multi-level feature extractor. The 5 × 5 filters have a larger receptive size and can learn more abstract features. The 1 × 1 filters are by definition local. The 3 × 3 filters sit as a balance in between.\n","\n","### Inception\n","\n","<img src=\"https://drive.google.com/uc?id=1ArR6-4wf_8ktnb9z7qgTlvjdOsA0ubdm\">\n","\n","- The module has four branches, coming from previous layer.\n","- The first branch in the inception module simply learns a series of 1x1 local features from the input.\n","- The second batch first applies 1x1 convolution, not only as a form of learning local features, but instead ass dimensionality reduction. \n","- If we can reduce the dimensionality of the inputs to these larger filters by applying 1 × 1 convolutions, we can reduce the amount of computation required by our network. Therefore, the number of filters learned in the 1 × 1 CONV in the second branch will always be smaller than the number of 3 × 3 filters learned directly afterward.\n","- The third branch applies the same logic as the second branch, only this time with the goal of learning 5 × 5 filters. We once again reduce dimensionality via 1 × 1 convolutions, then feed the output into the 5 × 5 filters.\n","- The fourth and the final branch of the Inception module performs 3x3 max pooling with a stride of 1x1 - this branch is called pool projection branch. \n","- Historically, models that perform pooling have demonstrated an ability to obtain higher accuracy, although we now know that this is not true and that POOL layers can be replace with CONV layers for reducing volume size.\n","- All four branches of the Inception module converge where they are concatenated together along the channel dimension.\n","- Special care is taken during the implementation (via zero padding) to\n","ensure the output of each branch has the same volume size, thereby allowing the outputs to be concatenated.\n","- The output of the Inception module is then fed into the next layer.\n","\n","### Miniception\n","\n","<img src=\"https://drive.google.com/uc?id=16ovYZqc6hjCV8R-qgpv72te-vFB7wsNH\" width>\n","\n","- The top row describes trhee modules used in their implementation\n","- **Left**: A convolution module responsible for performing convolution, batch normalization and activation.\n","- **Middle**: Miniception module which performs two sets of convolutions, one for 1x1 filters and other for 3x3 filters, then concatenates the results. No dimensionality reduction is performed before 3x3 filter as\n","  - The input volumes will be smaller already \n","  - To reduce the number number of parameters in network.\n","- **Right**: A downsample module which applies both a convolution and max pooling to reduce dimensionality, then concatenates across the filter dimension.\n","- These blocks are used to build MiniGoogleNet architecture shown in the image.\n","- In the implementation here, Batch Normalization is placed before activation, as given in the original paper."]},{"cell_type":"code","metadata":{"id":"q0QMVnRF-SaX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}