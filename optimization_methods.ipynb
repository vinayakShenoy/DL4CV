{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimization_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwGE/K1VfaAFkiCKFSNvJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayakShenoy/DL4CV/blob/master/optimization_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpuWoxw7-5dC",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Optimization Methods\n",
        "- So far we have used only SGD.\n",
        "- More advanced optimization techniques help to:\n",
        "  - Reduce the amount of time to obtain reasonable classification accuracy\n",
        "  - Make the network more well-behaved for a larger range of hyperparameters other than the learnng rate.\n",
        "  - Ideally, obtain higher classification accuracy than what is possible with SGD\n",
        "- SGD modifies all parameters in a network equally in proportion to a given learning rate\n",
        "- However, given that the learning rate of a network is the most important hyperparameter to tune and a hard, tedious hyperparameter to set correctly, it has been that it is possible to adaptively tune the learning rate as the network trains.\n",
        "\n",
        "## SGD\n",
        "\n",
        "$$\n",
        "W += -lr * dW\n",
        "$$\n",
        "- $W$: Weight matrix\n",
        "- $lr$: learning rate\n",
        "- $dW$: the gradient of W\n",
        "\n",
        "## Adagrad\n",
        "- Adagrad adapts the learning rate to the network parameters\n",
        "- Larger updates are performed on parameters that change infrequently while smaller updates are done on parameters that change frequently\n",
        "\n",
        "$$\n",
        "cache += (dW**2)\n",
        "$$\n",
        "$$\n",
        "W += -lr * dW/(np.sqrt(cache)+eps)\n",
        "$$\n",
        "\n",
        "- The cache variable maintains the per-parameter sum of squared gradients and is updated at every mini-batch in the training process.\n",
        "- By examining the cache, we can see which parameters are updated frequently and which ones are updated infrequently.\n",
        "- Scaling the update by the cache allows us to adaptively update the parameters in our network.\n",
        "- Weights that have frequently updated/large gradients in the cache will scale the size of the update down, effectively lowering the learning rate for the parameter. On the other hand, weights that have infrequent updates/smaller gradients in the cache will scale up the size of the update, effectively raising the learning rate for the specific parameter.\n",
        "- The primary benefit of Adagrad is that we no longer have to manually tune the learning rate – most implementations of the Adagrad algorithm leave the initial learning rate at 0.01 and allow the adaptive nature of the algorithm to tune the learning rate on a per-parameter basis.\n",
        "- The weakness of Adagrad is as follows. Since the gradients are squared, this accumulation keeps growing during the training process. dividing a small number (the gradient) by a very large number (the cache) will result in an update that is infinitesimally small, too small for the network to actually learn anything in later epochs.\n",
        "\n",
        "## Adadelta\n",
        "- In the Adagrad algorithm, we update our cache with all of the previously squared gradients.\n",
        "- However, Adadelta restricts this cache update by only accumulating a small number of past gradients\n",
        "- When actually implemented, this operation amounts to computing a decaying average of all past squared gradients.\n",
        "\n",
        "## RMSprop\n",
        "- Similar to Adadelta, RMSprop attempts to rectify the negative effects of a globally accumulated cache by converting the cache into an exponentially weighted moving average.\n",
        "\n",
        "$$\n",
        "cache = decay_raate*cache + (1-decay_rate)*(dW**2)\n",
        "$$\n",
        "$$\n",
        "W += -lr  * dW/(np.sqrt(cache)+eps)\n",
        "$$\n",
        "- The $decay_rate$, often defined as $\\phi$ is a hyperparameter typically set to 0.9.  \n",
        "\n",
        "## Adam\n",
        "- The Adam (Adaptive Moment Estimation) optimization algorithm, proposed by Kingma and Ba in their 2014 paper.\n",
        "- In practice, Adam tends to work better than RMSprop in many situations\n",
        "$$\n",
        "m = \\beta_1*m + (1-\\beta_1)*dW\n",
        "$$\n",
        "$$\n",
        "v = \\beta_2*v + (1-\\beta_2)*(dW**2)\n",
        "$$\n",
        "$$\n",
        "W += -lr*m/(np.sqrt(v)+\\epsilon)\n",
        "$$\n",
        "- The values of both m and v are similar to SGD momentum, relying on their respective previous values from time t − 1. The value m represents the first moment (mean) of the gradients while v is the second moment (variance).\n",
        "\n",
        "## Choosing an Optimization Method\n",
        "- Three methods to learn - **SGD, Adam, RMSprop**\n",
        "- Choosing an optimization algorithm to train a deep neural network is highly dependent on your familiarity with:\n",
        "  - The dataset\n",
        "  - The model architecture\n",
        "  - The optimization algorithm((and associated hyperparameters))\n",
        "\n",
        "# Optimal Pathway to Apply Deep Learning\n",
        "- The four ingredients to the recipe included:\n",
        "  - Your dataset\n",
        "  - A loss function\n",
        "  - A neural network architecture\n",
        "  - An optimization method\n",
        "- Take excruciating care to make sure your training data is representative of your validation and testing sets\n",
        "- There is no shortcut to building your own image dataset. If you expect a deep learning system to obtain high accuracy in a given real-world situation, then make sure this deep learning system was trained on images representative of where it will be deployed.\n",
        "<img src=\"https://drive.google.com/uc?id=17yhl2uflADQxUPQmjob92p8BG3rbmakV\" width=500px>\n",
        "- Based on the figure above we can see that Ng is proposing four sets of data splits when training a deep learning model:\n",
        "  - Training\n",
        "  - Training-validation (which Ng refers to as “development”)\n",
        "  - Validation\n",
        "  - Testing\n",
        "- If our training error is too high\n",
        "  - Then we should consider deepening our current architecture by adding in more layers and neurons. \n",
        "  - We should also consider training for longer (i.e., more epochs) while simultaneously tweaking our learning rate – using a smaller learning rate may enable you to train for longer while helping prevent overfitting.\n",
        "  - if after many experiments using our current architecture and varying learning rates does not prove useful, then we likely need to try an entirely different model architectu\n",
        "- If our training-validation error is high:\n",
        "  - we should examine the regularization parameters in our network. Are we\n",
        "applying dropout layers inside the network architecture? Is data augmentation being used to help generate new training samples? What about the actual loss/update function itself – is a regularization penalty being included? Examine these questions in the context of your own deep learning experiments and start adding in regularization.\n",
        "  - It is likely that your model does not have enough training data\n",
        "to learn the underlying patterns in your example images. \n",
        "  - After exhausting these options, you’ll once again want to consider using a different network architecture.\n",
        "- If our training-validation error is low, but our validation set error is high, \n",
        "  - We need to examine our training data with a closer eye. Are we absolutely, positively sure that our training images are similar to our validation images?\n",
        "  - Without data representative of where your deep learning model\n",
        "will be deployed, you will not obtain high accuracy results.\n",
        "- If our testing error is too high\n",
        " - we’ve overfit our model to the training and validation data\n",
        "\n",
        "## Tranfer Learning or Train from scratch\n",
        "- [Andrej Karpathy. Transfer Learning](http://cs231n.github.io/transfer-learning/)\n",
        "- To make this decision, you need to consider two important factors:\n",
        "  - The size of your dataset.\n",
        "  - The similarity of your dataset to the dataset the pre-trained CNN was trained on\n",
        "<img src=\"https://drive.google.com/uc?id=1McOX6XdFnQ3NDigHRF7N6IJwc7V4YTum\">\n",
        "\n",
        "### Dataset is small and similar to original dataset:\n",
        "- You'll likely don't have enough training examples to train a CNN from scratch(keep in mind we should ideally have 1000-5000 examples per class you want classify).\n",
        "- Furthermore given the lack of training data, its likely not a good idea to attempt fine-tuning as we'll likely end up overfitting.\n",
        "- since your image dataset is similar to what the pre-trained network was trained on, you should treat the network as a feature extractor and train a simple machine learning classifier on top of these features. \n",
        "- You should extract features from layers deeper in the architecture as these\n",
        "features are more rich and representative of the patterns learned from the original dataset\n",
        "\n",
        "### Dataset is Large and similar to original dataset\n",
        "- With a large dataset, we should have enough examples to apply fine-tuning without overfitting.\n",
        "- You may be tempted to train your own model from scratch here as well – this is an experiment worth running. \n",
        "- However, since your dataset is similar to the original dataset the network was\n",
        "already trained on, the filters inside the network are likely already discriminative enough to obtain a reasonable classifier.\n",
        "- Apply fine-tuning in this case.\n",
        "\n",
        "### Dataset is small and different than original dataset\n",
        "- Given a small dataset, we likely won’t obtain a high accuracy deep learning model by training from scratch.\n",
        "- We should again apply feature extraction and train a standard\n",
        "machine learning model on top of them – but since our data is different from the original dataset, we should use lower level layers in the network as our feature extractors.\n",
        "- Keep in mind that the deeper we go into the network architecture, the more rich and discriminative the features are specific to the dataset it was trained on. By extracting features from lower layers in the network, we can still leverage these filters, but without the abstraction caused by the deeper layers.\n",
        "\n",
        "### Dataset is large and different than original dataset\n",
        "- We have two options. \n",
        "  - Given that we have sufficient training data, we can likely train our own custom network from scratch. \n",
        "  - However, the pre-trained weights from models trained on dataset such as ImageNet make for excellent initializations, even if the datasets are unrelated.\n",
        "- We should therefore perform two sets of experiments:\n",
        "  - In the first set of experiments, attempt to fine-tune a pre-trained network to your dataset and evaluate the performance.\n",
        "  - Then in the second set of experiments, train a brand new model from scratch and evaluate.\n",
        "- Try to fine-tune first as this method will allow you to establish a baseline to beat when you move on to your second set of experiments and train your network from scratch.\n",
        "\n",
        "# References\n",
        "- [Geoffrey Hinton. Neural Networks for Machine Learning.](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n",
        "- [Andrej Karpathy. Neural Networks (Part III).](https://cs231n.github.io/neural-networks-3/)\n",
        "- [Andrew Ng. Nuts and Bolts of Building Applications using Deep Learning](https://nips.cc/Conferences/2016/Schedule?showEvent=6203)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibLFmPvL-0QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}