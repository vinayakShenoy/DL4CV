{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensembles.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfcDHZCM6+zOUFA8dGFRra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayakShenoy/DL4CV/blob/master/ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRTLSnctlttE",
        "colab_type": "text"
      },
      "source": [
        "# Improving Accuracy withe ensembles\n",
        "- The term “ensemble methods” generally refers to training a “large” number of models (where the exact value of “large” depends on the classification task) and then combining their output predictions via voting or averaging to yield an increase in classification accuracy.\n",
        "- By averaging multiple machine learning models together, we can outperform using just a single model chosen at random. \n",
        "- Like in Random Forests, where we train multiple Decision Trees, here we train multiple networks and then ask each network to return the probabilities for each class label given an input data point. These probabilities are averaged together and the final classification is obtained.\n",
        "\n",
        "## Jensen's Inequality\n",
        "- The formal definition of Jensen’s Inequality states that the convex combined(average) ensemble will have error less than or equal to the average error of the individual models.\n",
        "---\n",
        "## Constructing an ensemble of CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1YGWt5xpBb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "32f421d4-8dc7-4dc4-c1e9-755d181b22ba"
      },
      "source": [
        "!pip install import_ipynb\n",
        "!git clone https://github.com/vinayakShenoy/DL4CV\n",
        "%cd DL4CV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=34e56a46f5300ed6465d92b09e9201ccfc7f09d0170002977e91d93e5460d842\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Cloning into 'DL4CV'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 119 (delta 35), reused 37 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (119/119), 3.24 MiB | 5.69 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "/content/DL4CV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-wWe3QIlqkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e36af72e-9422-41e7-ce54-025b1fffcaab"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import import_ipynb\n",
        "from pyimage.nn.MiniVGGNet import MiniVGGNet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/DL4CV/pyimage/nn/MiniVGGNet.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNwYOBTgn8Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"output\":\"output\",\n",
        "    \"models\":\"models\",\n",
        "    \"num-models\":5\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJe4OeEHoCaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8284866d-c852-4632-ef75-ca4dbb3c3fef"
      },
      "source": [
        "# load the training and testing data, then scale it into the range [0,1]\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\")/255.0\n",
        "testX = testX.astype(\"float\")/255.0\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.fit_transform(testY)\n",
        "\n",
        "# init the label names for cifar10\n",
        "labelNames = [\"airplanes\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\",\"truck\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJgR9hygow96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1fvGgFpXVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in np.arange(0, args[\"num-models\"]):\n",
        "  # init the optimizer and model\n",
        "  print(\"INFO training model {}/{}\".format(i+1, args[\"num-models\"]))\n",
        "  opt = SGD(lr=0.01, decay=0.01/40, momentum=0.9, nesterov=True)\n",
        "  model = MiniVGGNet.build(width=32, height=32, depth=3, classes=10)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "  # train the network\n",
        "  H = model.fit(aug.flow(trainX, trainY, batch_size=64),\n",
        "                validation_data=(testX, testY), epochs=40,\n",
        "                steps_per_epoch=len(trainX)//64, verbose=1)\n",
        "  \n",
        "  # save the mode to disk\n",
        "  p = [args[\"models\"], \"model_{}.model\".format(i)]\n",
        "  model.save(os.path.sep.join(p))\n",
        "\n",
        "  # evaluate the network\n",
        "  predictions = model.predict(testX, batch_size=64)\n",
        "  report = classification_report(testY.argmax(axis=1), \n",
        "                                 predictions.argmax(axis=1), target_names=labelNames)\n",
        "\n",
        "  # save the classification report to file\n",
        "  p = [args[\"output\"], \"model_{}.txt\".format(i)]\n",
        "  f = open(os.path.sep.join(p), \"w\")\n",
        "  f.write(report)\n",
        "  f.close()\n",
        "\n",
        "  # plot the training loss and accuracy\n",
        "  p = [args[\"output\"], \"model_{}.png\".format(i)]\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(0, 40), H.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(np.arange(0, 40), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(np.arange(0, 40), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "  plt.plot(np.arange(0, 40), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy for model {}\".format(i))\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.sep.join(p))\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK79t6Hqrqg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(testX, testY) = cifar10.load_data()[1]\n",
        "testX = testX.astype(\"float\")/255.0\n",
        "\n",
        "# init labelNames \n",
        "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# convert the labels from the integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "testY = lb.fit_transform(testY)\n",
        " \n",
        "# construct path used to collect models then initializer the models list\n",
        "modelPaths = os.path.sep.join([args[\"models\"], \"*.model\"])\n",
        "modelPaths = list(glob.glob(modelPaths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7c6iBCe9NuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = []\n",
        "\n",
        "# loop over model paths, load each model and add it to list of models\n",
        "for (i, modelPath) in enumerate(modelPaths):\n",
        "  print(\"INFO loading model {}/{}\".format(i+1,\n",
        "                                          len(modelPaths)))\n",
        "  models.append(load_model(modelPaths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu_aEvgQ9xWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init the list of predictions\n",
        "print(\"INFO evaluating ensemble\")\n",
        "predictions = []\n",
        "\n",
        "for model in models:\n",
        "  # us curr model to make predcitions on testing data \n",
        "  #then store these predictions in the aggregate predictions list\n",
        "  predictions.append(model.predict(testX, batch_size=64))\n",
        "\n",
        "# prediictions will be of shape (5,10000, 10)\n",
        "# where 5 is number of models, 10000 is number of images and 10 is probability per\n",
        "# class.\n",
        "predictions = np.average(predictions, axis=0)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1), target_names=labelNames))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}