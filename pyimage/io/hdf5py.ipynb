{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vinayakShenoy/DL4CV/blob/master/Code/pyimage/io/hdf5py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-7OLe_W9C6Q"
   },
   "source": [
    "# Feature Extraction\n",
    "- Next few chapters will be on the concept of transfer learning, the ability to use a pretrained model as a shortcut to learn patterns from data it was not originally trained on.\n",
    "- There are two types of transfer learning when applied to deep learning for computer vision:\n",
    "  - Tresting networks as arbitrary feature extractors.\n",
    "  - Removing the fully-connected layers of an existing network, placing new FC layer set on top of CNN and fine-tuning these weights(and optionally previous layers) to recognize object classes\n",
    "- This chapter, we discuss first method, i.e, treating networks as feature extractors.\n",
    "\n",
    "## Extracting features with a Pre-trained CNN\n",
    "- Usually, we treat CNN as end-to-end image classifiers.\n",
    "  - We input an image to the network\n",
    "  - The image forward propagates through the network.\n",
    "  - We obtain the final classification probabilities from the end of the network.\n",
    "- We can stop the propagation at an arbitrary layer and extract the values at this time and use them as feature vectors, that quantifies the contents of an image.\n",
    "- If this repeated for an entire dataset, we cana train SVM, Logistic regression, or random forest on top of these features to obtain a classifier that recognizer new class of images.\n",
    "-the trick is extracting these features and storing them in an efficient manner. To accomplish this task, we’ll need HDF5.\n",
    "\n",
    "## HDF5\n",
    "- HDF5 is a binary data format used to store gigantic datasets on disk while facilitating easy access and computation on rows of the datasets. \n",
    "- Written in C, but we can gain access to C API using Python library, allowing us store huge amounts of data in our HDF5 dataset and manipulate the data in a numpy-like fashion.\n",
    "- Datasets stored in HDF5 format is portable and can be accessed in C, Matlab and java.\n",
    "- Below we will write a custom python class that allows us to efficiently accept input data and write it to HDF5 dataset. \n",
    "  - Facilitate method to apply transfer learning by taking extracted features from VGG16 and writing them to HDF5.\n",
    "  - Allow us to generate HDF5 datasets from raw images to facilitate faster training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "381-yoHwWhfu",
    "outputId": "a5147fbc-39c3-4e31-af96-0cc314fb21bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gk7tPKir84a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1X3Fs3gOL-A"
   },
   "outputs": [],
   "source": [
    "class HDF5DatasetWriter:\n",
    "  # dims parameter control dimensions of data that will be stored in dataste.\n",
    "  #     If we are storing flatten raw pixel intensities of 28x28=784, for 70000 examples then dims=(70000, 784)\n",
    "  #     If we store raw CIFAR10 (unflattened), dims=(60000, 32, 32, 3)\n",
    "  #     In context of feature extraction, if the final POOL layer is 512x7x7 when flatten, it is a feature vector of length 25088. We set dims=(N, 25088) \n",
    "  #     where N is number of images in dataset\n",
    "  # datakey indicates that we are storing extracted features from CNN.\n",
    "  # bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset\n",
    "\n",
    "  def __init__(self, dims, outputPath, dataKey=\"images\", bufSize=1000):\n",
    "    #check to see if output path exists\n",
    "    if os.path.exists(outputPath):\n",
    "      raise ValueError(\"Path exists\", outputPath)\n",
    "\n",
    "    #copen hdf5 db for writing and create two datasets\n",
    "    #one to store images and another to store class labels\n",
    "    self.db = h5py.File(outputPath, \"w\")\n",
    "    self.data = self.db.create_dataset(dataKey, dims, dtype=\"float\")\n",
    "    self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n",
    "\n",
    "    self.bufSize = bufSize\n",
    "    self.buffer = {\"data\":[], \"labels\":[]}\n",
    "    self.idx = 0\n",
    "  \n",
    "  def add(self, rows, labels):\n",
    "    # add rows and labels to the buffer\n",
    "    self.buffer[\"data\"].extend(rows)\n",
    "    self.buffer[\"labels\"].extend(labels)\n",
    "\n",
    "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
    "      self.flush()\n",
    "  \n",
    "  def flush(self):\n",
    "    # write buffers to disk then reset buffer\n",
    "    i = self.idx + len(self.buffer[\"data\"])\n",
    "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "    self.idx = i\n",
    "    self.buffer = {\"data\":[], \"labels\":[]}\n",
    "\n",
    "  def storeClassLabels(self, classLabels):\n",
    "    # create a dataset to store the actual class label  names,\n",
    "    # then store the class labels\n",
    "    dt = h5py.special_dtype(vlen=unicode)\n",
    "    labelSet = self.db.create_dataset(\"label_names\", (len(classLabels),), dtype=dt)\n",
    "    labelSet[:] = classLabels\n",
    "\n",
    "  def close(self):\n",
    "    # check if there are any entries in buffer \n",
    "    # that need to be flushed to disk\n",
    "    if len(self.buffer[\"data\"])>0:\n",
    "      self.flush()\n",
    "      self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5DatasetGenerator:\n",
    "  def __init__(self, dbPath, batchSize, preprocessors=None,\n",
    "    aug=None, binarizer=True, classes=2):\n",
    "    self.batchSize = batchSize\n",
    "    self.preprocessors = preprocessors\n",
    "    self.aug = aug\n",
    "    self.binarize = binarize\n",
    "    self.classes = classes\n",
    "        \n",
    "    self.db = h5py.File(dbPath,\"r\")\n",
    "    self.numImages = self.db[\"labels\"].shape[0]\n",
    "        \n",
    "  def generator(self, passes=np.inf):\n",
    "    epochs = 0\n",
    "    while epochs<passes:\n",
    "      for i in np.arange(0, self.numImages, self.batchSize):\n",
    "        images = self.db[\"images\"][i:i+self.batchSize]\n",
    "        labels = self.db[\"labels\"][i:i+self.batchSize]\n",
    "\n",
    "        if self.binarize:\n",
    "          labels = to_categorical(labels, self.classes)\n",
    "\n",
    "        if self.preprocessors is not None:\n",
    "          procImages = []\n",
    "          for image in images:\n",
    "            for p in self.preprocessors:\n",
    "              image = p.preprocess(image)\n",
    "\n",
    "            procImages.append(image)\n",
    "          images = np.array(procImages)\n",
    "\n",
    "        if self.aug is not None:\n",
    "          (images, labels) = next(self.aug.flow(images, labels,\n",
    "                                                batch_size=self.batchSize))\n",
    "\n",
    "        yield (images, labels)\n",
    "\n",
    "      epochs += 1\n",
    "    \n",
    "  def close(self):\n",
    "    self.db.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9CywE3xnHcBtn15014mAg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "hdf5py.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
